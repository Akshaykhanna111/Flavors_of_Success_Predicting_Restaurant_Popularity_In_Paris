{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "In this project we will be using Open data to predict the restaurant popularity in Paris. Following are the data points taken into account for analysis - \n",
    "\n",
    "1. Restaurant Data - Yelp API (500 calls per user in 24 hours) giving details like restaurant ratings, review count etc.\n",
    "2. Goole Maps Places Data - Details like reviews and ratings of neighborhood vicinity points like - shopping malls, grocery stroes, museums, public transit etc. \n",
    "3. Bike Stations - Velib data points around number of bike stations, available bikes etc. in vicinity of 1km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import ttest_ind\n",
    "import sqlite3\n",
    "import googlemaps\n",
    "\n",
    "# Check for the installation of required packages and install if necessary\n",
    "required_packages = [\"requests\", \"pandas\", \"plotly\", \"seaborn\", \"scipy\"]\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Bike API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection \n",
    "# Fetch the City Bikes API Data\n",
    "request = requests.get('http://api.citybik.es/v2/networks')\n",
    "\n",
    "# Check for success or failure\n",
    "#print(request.status_code)\n",
    "\n",
    "# Observe the records\n",
    "#print(json.dumps(request.json(), sort_keys=True, indent=4))\n",
    "\n",
    "# Ask the user to select a city of choice for analysis \n",
    "\n",
    "# Create a dictionary of href and city. \n",
    "# City is required for user input and href is required for endpoint to fetch bike station data\n",
    "\n",
    "dictionary_of_cities_href = {'list_of_cities': [],\n",
    "                            'href': []}\n",
    "\n",
    "for i in range(len(request.json()['networks'])):\n",
    "    dictionary_of_cities_href['list_of_cities'].append(request.json()['networks'][i]['location']['city'])\n",
    "    dictionary_of_cities_href['href'].append(request.json()['networks'][i]['href'])\n",
    "\n",
    "# Print the list of cities for user to select \n",
    "user_choice = input(f\"Select a city to fetch the data for:\\n{', '.join(dictionary_of_cities_href['list_of_cities'])}\\n\")\n",
    "\n",
    "pattern = re.compile(user_choice, re.IGNORECASE)\n",
    "\n",
    "# Filter the list based on the regex match\n",
    "matching_strings = [s for s in dictionary_of_cities_href['list_of_cities'] if pattern.search(s)]\n",
    "    \n",
    "print(f\"You have selected - {matching_strings[0]}\")\n",
    "\n",
    "# Create and use the endpoint for the user input city to fetch the bike station data\n",
    "\n",
    "endpoint = 'https://api.citybik.es/'+ dictionary_of_cities_href['href'][dictionary_of_cities_href['list_of_cities'].index(matching_strings[0])]\n",
    "\n",
    "request_station_date = requests.get(endpoint)\n",
    "\n",
    "#print(request_station_date.status_code)\n",
    "\n",
    "#print(json.dumps(request_station_date.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the details you want for the bike stations in that city (latitude, longitude, number of bikes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Fields \n",
    "# Station_Id, Station_Name, Latitude, Longitude, Timestamp, City, Country, Total_Slots, Total Available_Bikes, Available Ebikes, \n",
    "# Available Normal Bikes\n",
    "\n",
    "# Below is a blank dict and the comments comprise of nesting level for each relevant details within the json response.\n",
    "# Viewing the same in Notepad++ or using Pprint will make it easier to study the data. \n",
    "bike_dataframe_dictionary = {\n",
    "    'Station_Id': [], # network (d) -> stations (ld) -> id (k)\n",
    "    'Station_Name': [], # network (d) -> stations (ld) -> name (k)\n",
    "    'Latitude': [], # network (d) -> stations (ld) -> latitude (k)\n",
    "    'Longitude': [], # network (d) -> stations (ld) -> longitude (k) \n",
    "    'Timestamp': [], # network (d) -> stations (ld) -> timestamp (k)  \n",
    "    'City': [], # network (d) -> location (d) -> city (k)\n",
    "    'Country': [], # network (d) -> location (d) -> country (k)\n",
    "    'Total_Available_Slots': [], # network (d) -> stations (ld) -> extra (d) -> slots (k)\n",
    "    'Total_Available_Free_Bikes': [], # network (d) -> stations (ld) -> free_bikes (k)\n",
    "    'Total_Available_EBikes': [], # network (d) -> stations (ld) -> extra (d) -> ebikes (k)\n",
    "    #'Total_Available_Normal_Bikes': [], # network (d) -> stations (ld) -> extra (d) -> normal_bikes (k)\n",
    "    'Total_Available_Empty_Slots': [] # network (d) -> stations (ld) -> empty_slots (k)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in request_station_date.json()['network'].keys():\n",
    "    if 'stations' in i:\n",
    "        for j in range(len(request_station_date.json()['network'][i])):\n",
    "            for k in request_station_date.json()['network'][i][j].keys():\n",
    "                if 'id' == k:\n",
    "                    bike_dataframe_dictionary['Station_Id'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'name' == k:\n",
    "                    bike_dataframe_dictionary['Station_Name'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'latitude' == k:\n",
    "                    bike_dataframe_dictionary['Latitude'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'longitude' == k:\n",
    "                    bike_dataframe_dictionary['Longitude'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'timestamp' == k:\n",
    "                    bike_dataframe_dictionary['Timestamp'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'empty_slots' == k:\n",
    "                    bike_dataframe_dictionary['Total_Available_Empty_Slots'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'free_bikes' == k:\n",
    "                    bike_dataframe_dictionary['Total_Available_Free_Bikes'].append(request_station_date.json()['network'][i][j][k])\n",
    "                if 'extra' == k:\n",
    "                    for m in request_station_date.json()['network'][i][j][k].keys():\n",
    "                        if 'slots' == m:\n",
    "                            bike_dataframe_dictionary['Total_Available_Slots'].append(request_station_date.json()['network'][i][j][k][m])\n",
    "                        if 'ebikes' == m:\n",
    "                            bike_dataframe_dictionary['Total_Available_EBikes'].append(request_station_date.json()['network'][i][j][k][m])\n",
    "                        # if 'normal_bikes' == m:\n",
    "                        #     bike_dataframe_dictionary['Total_Available_Normal_Bikes'].append(request_station_date.json()['network'][i][j][k][m])\n",
    "    if 'location' == i:\n",
    "        for l in request_station_date.json()['network'][i].keys():\n",
    "            if 'city' == l:\n",
    "                bike_dataframe_dictionary['City'].append(request_station_date.json()['network'][i][l])\n",
    "            if 'country' == l:\n",
    "                bike_dataframe_dictionary['Country'].append(request_station_date.json()['network'][i][l])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Put your parsed results into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bike_dataframe_dictionary)\n",
    "bike_dataframe_city_country = pd.DataFrame({'id': 1, \n",
    "                                           'City': bike_dataframe_dictionary['City'],\n",
    "                                           'Country': bike_dataframe_dictionary['Country']})\n",
    "bike_dataframe_station_details = pd.DataFrame({'id': 1, \n",
    "                                               'Station_Id': bike_dataframe_dictionary['Station_Id'],\n",
    "                                                'Station_Name': bike_dataframe_dictionary['Station_Name'],\n",
    "                                                'Latitude': bike_dataframe_dictionary['Latitude'],\n",
    "                                                'Longitude': bike_dataframe_dictionary['Longitude'], \n",
    "                                                'Timestamp': bike_dataframe_dictionary['Timestamp'],  \n",
    "                                                'Total_Available_Slots': bike_dataframe_dictionary['Total_Available_Slots'],\n",
    "                                                'Total_Available_Free_Bikes': bike_dataframe_dictionary['Total_Available_Free_Bikes'],\n",
    "                                                'Total_Available_EBikes': bike_dataframe_dictionary['Total_Available_EBikes'],\n",
    "                                                #'Total_Available_Normal_Bikes': bike_dataframe_dictionary['Total_Available_Normal_Bikes'],\n",
    "                                                'Total_Available_Empty_Slots': bike_dataframe_dictionary['Total_Available_Empty_Slots']})\n",
    "\n",
    "bike_dataframe_city_country = pd.DataFrame({'id': 1, \n",
    "                                           'City': bike_dataframe_dictionary['City'],\n",
    "                                           'Country': bike_dataframe_dictionary['Country']})\n",
    "bike_dataframe_station_details = pd.DataFrame({'id': 1, \n",
    "                                               'Station_Id': bike_dataframe_dictionary['Station_Id'],\n",
    "                                                'Station_Name': bike_dataframe_dictionary['Station_Name'],\n",
    "                                                'Latitude': bike_dataframe_dictionary['Latitude'],\n",
    "                                                'Longitude': bike_dataframe_dictionary['Longitude'], \n",
    "                                                'Timestamp': bike_dataframe_dictionary['Timestamp'],  \n",
    "                                                'Total_Available_Slots': bike_dataframe_dictionary['Total_Available_Slots'],\n",
    "                                                'Total_Available_Free_Bikes': bike_dataframe_dictionary['Total_Available_Free_Bikes'],\n",
    "                                                'Total_Available_EBikes': bike_dataframe_dictionary['Total_Available_EBikes'],\n",
    "                                                #'Total_Available_Normal_Bikes': bike_dataframe_dictionary['Total_Available_Normal_Bikes'],\n",
    "                                                'Total_Available_Empty_Slots': bike_dataframe_dictionary['Total_Available_Empty_Slots']})\n",
    "bike_dataframe = pd.merge(bike_dataframe_city_country, bike_dataframe_station_details, on = 'id', how = 'outer')\n",
    "print(bike_dataframe.shape)\n",
    "bike_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bike_dataframe.to_csv('City_Bike_Paris_Data_9th.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Google MAPS API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googlemaps\n",
    "#import googlemaps\n",
    "\n",
    "# Function to check the googlemaps version\n",
    "def main():\n",
    "    print(f\"googlemaps version: {googlemaps.__version__}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Load the Google Maps Key\n",
    "google_maps_key = os.getenv('Google_Places_API_Key')\n",
    "\n",
    "# Initialize Google Maps Client with API Key\n",
    "gmaps = googlemaps.Client(key=google_maps_key)\n",
    "\n",
    "# Set Pandas display options - To view the complete data post execution of code\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None) \n",
    "\n",
    "# Read data\n",
    "city_bike_df = pd.read_csv(r'City_Bike_Paris_Data_9th.csv')\n",
    "sample_df = city_bike_df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "\n",
    "# Function used for parsing the categories returned in API response\n",
    "def check_matching_value(lst, value):\n",
    "    return value in lst\n",
    "\n",
    "# Function to fetch results from Google Maps API and append to a list\n",
    "def fetch_gmaps_results(lat, long, df_list):\n",
    "    df_check = pd.DataFrame()\n",
    "    radius = 1000\n",
    "    data = gmaps.places_nearby((lat, long), radius)\n",
    "    if data['status'] == 'OK':\n",
    "        df_check = pd.DataFrame(pd.json_normalize(data['results']))\n",
    "        df_check['latitude'] = lat\n",
    "        df_check['longitude'] = long\n",
    "        selected_columns = ['place_id', 'latitude', 'longitude', 'price_level', 'rating', 'user_ratings_total', 'types']\n",
    "        for col in selected_columns:\n",
    "            if col not in df_check.columns:\n",
    "                df_check[col] = np.nan\n",
    "        df_check = df_check[selected_columns]\n",
    "        list_POI = ['lodging', 'airport', 'library', 'amusement_park', 'light_rail_station', 'aquarium', 'bus_station', 'casino', 'shopping_mall', 'stadium', 'subway_station', 'tourist_attraction', 'train_station', 'transit_station']\n",
    "        for search_value in list_POI:\n",
    "            df_check[search_value] = df_check['types'].apply(lambda x: check_matching_value(x, search_value))\n",
    "        df_list.append(df_check)\n",
    "\n",
    "# Initialize an empty list for storing DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop over each row in sample_df to fetch Google Maps data\n",
    "for index, row in sample_df.iterrows():\n",
    "    fetch_gmaps_results(row['Latitude'], row['Longitude'], df_list)\n",
    "\n",
    "# Concatenate all DataFrames in the list into df_final\n",
    "df_final = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing can be done on df_final as needed\n",
    "\n",
    "df_final.to_csv(r'Gmaps_Data_9th.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YELP API CALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for API Call to Yelp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get restaurant ratings\n",
    "\n",
    "response_variable = ''\n",
    "\n",
    "def get_restaurant_ratings(lat, lng, api_key):\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "    headers = {'Authorization': 'Bearer %s' % api_key}\n",
    "    params = {'latitude': lat, 'longitude': lng, 'limit': 50}\n",
    "    global response_variable\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response_variable = response\n",
    "        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "        data = pd.DataFrame(pd.json_normalize(response.json()['businesses']))\n",
    "        if not data.empty:\n",
    "            new_df = data[data.distance <= 1000]\n",
    "            new_df['Latitude'] = lat \n",
    "            new_df['Longitude'] = lng\n",
    "            return new_df\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"Http Error: {errh}\")\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 of Fetching the results (Yelp allows 500 API calls per key per user in 24 hours). We have 1462 data points so will have to run the code and save the datasets in batches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the initial bike data\n",
    "bike_dataframe = pd.read_csv(r'City_Bike_Paris_Data_9th.csv')\n",
    "bike_dataframe_lat_long = bike_dataframe[['Latitude', 'Longitude']].drop_duplicates()\n",
    "\n",
    "\n",
    "# Batch 1\n",
    "# Your Yelp API Key\n",
    "api_key = os.getenv('Yelp_API_Key_2')\n",
    "\n",
    "all_ratings = []  # List to store all ratings dataframes\n",
    "\n",
    "# Iterate over rows efficiently\n",
    "for _, row in bike_dataframe_lat_long.iterrows():\n",
    "    lat, lng = row['Latitude'], row['Longitude']\n",
    "    ratings_df = get_restaurant_ratings(lat, lng, api_key)\n",
    "    all_ratings.append(ratings_df)\n",
    "\n",
    "complete_ratings_df = pd.DataFrame()\n",
    "\n",
    "# Concatenate all results\n",
    "complete_ratings_df = pd.concat(all_ratings, ignore_index=True)\n",
    "print(complete_ratings_df.shape)\n",
    "complete_ratings_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "complete_ratings_df.to_csv(r'Yelp_Lat_Long_Matching_Dataset_Part1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 of Fetching the results (Yelp allows 500 API calls per key per user in 24 hours). We have ~900 data points so will have to run the code and save the datasets in batches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Part 1 dataset \n",
    "part_1_ratings_df = pd.read_csv(r'Yelp_Lat_Long_Matching_Dataset_Part1.csv', index=False)\n",
    "\n",
    "# Group by and count\n",
    "ratings_df_agg = part_1_ratings_df.groupby(['Latitude', 'Longitude']).size().reset_index(name='counts')\n",
    "\n",
    "# Merge with the bike data\n",
    "merged = bike_dataframe_lat_long.merge(ratings_df_agg, on=['Latitude', 'Longitude'], how='outer', indicator=True)\n",
    "\n",
    "# Create the set of remaining lat long pairs for which the API call needs to be done\n",
    "batch2_lat_long = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "batch2_lat_long.shape\n",
    "\n",
    "# Your Yelp API Key\n",
    "api_key = os.getenv('Yelp_API_Key_2')\n",
    "\n",
    "all_ratings = []  # List to store all ratings dataframes\n",
    "ratings_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over rows efficiently\n",
    "for _, row in batch2_lat_long.iterrows():\n",
    "    lat, lng = row['Latitude'], row['Longitude']\n",
    "    ratings_df = get_restaurant_ratings(lat, lng, api_key)\n",
    "    all_ratings.append(ratings_df)\n",
    "\n",
    "complete_ratings_df = pd.DataFrame()\n",
    "\n",
    "# Concatenate all results\n",
    "complete_ratings_df = pd.concat(all_ratings, ignore_index=True)\n",
    "print(complete_ratings_df.shape)\n",
    "complete_ratings_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "complete_ratings_df.to_csv(r'C:\\Path\\To\\Yelp_Lat_Long_Matching_Dataset_Part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 of Fetching the results (Yelp allows 500 API calls per key per user in 24 hours). We have 499 data points so will have to run the code and save the datasets in batches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path needs to be updated\n",
    "\n",
    "# Read the Part 1 and 2 dataset \n",
    "\n",
    "part_1_ratings_df = pd.read_csv(r'Yelp_Lat_Long_Matching_Dataset_Part1.csv')\n",
    "part_2_ratings_df = pd.read_csv(r'Yelp_Lat_Long_Matching_Dataset_Part2.csv')\n",
    "\n",
    "# Concatenate the 2\n",
    "merged_ratings_df = pd.concat([part_1_ratings_df, part_2_ratings_df], axis = 0)\n",
    "\n",
    "# Group by and count\n",
    "ratings_df_agg = merged_ratings_df.groupby(['Latitude', 'Longitude']).size().reset_index(name='counts')\n",
    "\n",
    "# Merge with the bike data\n",
    "merged = bike_dataframe_lat_long.merge(ratings_df_agg, on=['Latitude', 'Longitude'], how='outer', indicator=True)\n",
    "\n",
    "# Create the set of remaining lat long pairs for which the API call needs to be done\n",
    "batch3_lat_long = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "batch3_lat_long.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_ratings = []  # List to store all ratings dataframes\n",
    "\n",
    "# Iterate over rows efficiently\n",
    "for _, row in batch3_lat_long.iterrows():\n",
    "    lat, lng = row['Latitude'], row['Longitude']\n",
    "    ratings_df = get_restaurant_ratings(lat, lng, api_key)\n",
    "    all_ratings.append(ratings_df)\n",
    "\n",
    "\n",
    "#Concatenate all results\n",
    "complete_ratings_df = pd.concat(all_ratings, ignore_index=True)\n",
    "print(complete_ratings_df.shape)\n",
    "complete_ratings_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to CSV\n",
    "complete_ratings_df.to_csv(r'Yelp_Lat_Long_Matching_Dataset_Part3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
